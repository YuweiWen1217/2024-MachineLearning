{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [实验六报告](https://github.com/YuweiWen1217/2024-MachineLearning)\n",
    "文昱韦 2213125\n",
    "\n",
    "### 实验六：决策树分类器\n",
    "1. **初级要求**：基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "2. **中级要求**：对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "3. **高级要求**：使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初级要求\n",
    "\n",
    "#### 将离散属性映射为数值\n",
    "\n",
    "Watermelon-test1.csv数据集中，每个特征都是离散属性，我们需要将离散的特征值和类别标签映射到数值索引。\n",
    "\n",
    "`feature_dict` 定义了每个特征及其可能取值，`label_list` 定义了类别标签集合。\n",
    "\n",
    "例如，\"色泽\" 包括 \"青绿\"、\"乌黑\" 和 \"浅白\"，它们被映射为 0、1 和 2；再比如，标签中，\"不是好瓜\"被映射为0，\"是好瓜\"被映射为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "feature_dict = {\"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "                \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "                \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "                \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"]\n",
    "                }\n",
    "lable_list = [\"否\", \"是\"]\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\"]\n",
    "\n",
    "def load_txt(path):\n",
    "    ans = []\n",
    "    with codecs.open(path, \"r\", \"GBK\") as f:\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            d = line.rstrip(\"\\r\\n\").split(',')\n",
    "            re = []\n",
    "            # 输入编号方便追踪\n",
    "            re.append(int(d[0]))\n",
    "            # feature_dict.get(\"色泽\") = ['青绿', '乌黑', '浅白']\n",
    "            re.append(feature_dict.get(\"色泽\").index(d[1]))\n",
    "            re.append(feature_dict.get(\"根蒂\").index(d[2]))\n",
    "            re.append(feature_dict.get(\"敲声\").index(d[3]))\n",
    "            re.append(feature_dict.get(\"纹理\").index(d[4]))\n",
    "            re.append(lable_list.index(d[-1]))\n",
    "            ans.append(np.array(re))\n",
    "            line = f.readline()\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分选择：信息增益\n",
    "\n",
    "信息增益是 ID3 决策树算法选择最佳分裂特征的核心指标，用于衡量某个特征对数据集分类的贡献大小，公式为：  \n",
    "$$\n",
    "Gain(D, A) = Ent(D) - \\sum_{v \\in V} \\frac{|D_v|}{|D|} \\cdot Ent(D_v)\n",
    "$$\n",
    "其中，\n",
    "$$\n",
    "Ent(D) = - \\sum_{k=1}^{|y|}p_k \\log_2{p_k}\n",
    "$$\n",
    "- $ Ent(D) $ 是当前数据集合 $ D $ 的信息熵，值越小，$D$的纯度越高。\n",
    "- $ Ent(D_v)$是特征$ A $ 取值为$v $的子集的熵。\n",
    "- $ \\frac{|D_v|}{|D|} $ 是权重，表示$ D_v $ 在 $ D $中的占比。\n",
    "\n",
    "信息增益让我们知道，通过某个特征分割数据后，熵的下降量会有多大，越大则说明该特征越能有效区分数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent(D):\n",
    "    # D: 样本标签列表\n",
    "    # ENT(D) = - Σ P(k) * log2(P(k))\n",
    "    s = 0\n",
    "    for k in set(D):\n",
    "        p_k = np.sum(np.where(D == k, 1, 0)) / np.shape(D)[0]\n",
    "        if p_k == 0:\n",
    "            # 此时Pklog2Pk 定义为 0\n",
    "            continue\n",
    "        s += p_k * np.log2(p_k)\n",
    "    return -s\n",
    "\n",
    "def gain(X, Y, attr):\n",
    "    # return: 总的熵 - 各个分支的熵的加权平均值\n",
    "    # X, Y是样本及标签， attr是某个特征\n",
    "    x_attr_col = X[:, attr]\n",
    "    ent_Dv = []\n",
    "    weight_Dv = []\n",
    "    # 离散值处理\n",
    "    for x_v in set(x_attr_col):\n",
    "        index_x_equal_v = np.where(x_attr_col == x_v) # 特征attr的值等于x_v的样本编号\n",
    "        y_x_equal_v = Y[index_x_equal_v]    # 特征attr的值等于x_v的样本类别\n",
    "        ent_Dv.append(ent(y_x_equal_v))\n",
    "        weight_Dv.append(np.shape(y_x_equal_v)[0] / np.shape(Y)[0])\n",
    "    return ent(Y) - np.sum(np.array(ent_Dv) * np.array(weight_Dv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 决策树的节点设计及决策树的构建\n",
    "\n",
    "决策树上的每个节点表示决策树中的一个分支或叶节点：\n",
    "- attr：当前节点对应的划分特征索引，叶节点时为一个特定值。\n",
    "- label：叶节点的分类结果。非叶节点时为一个特定值。\n",
    "- attr_v：从父节点到当前节点的分支值。\n",
    "- children：子节点列表。\n",
    "\n",
    "参考西瓜书P74提供的算法，决策树的构建遵循“自顶向下递归分裂”的思想：\n",
    "1. 在当前节点，根据所求得的信息增益选择一个最佳特征，按特征的不同取值划分数据集。\n",
    "2. 对每个子数据集递归地构建子树。\n",
    "3. 如果满足终止条件（已无可用特征），则停止分裂，将当前节点设为叶节点。\n",
    "\n",
    "书上有三种递归返回的情况：\n",
    "- 如果当前数据集 Y 中所有样本的标签相同，则直接设为叶节点；\n",
    "- 如果特征用尽，或者数据集在剩余特征上取值相同，将Y中出现最多的标签值为叶节点标签；\n",
    "- 某个特征选出来后，没有对应的样本。这种情况在我们的代码中不存在，因为我们直接提取的真实数据集中的特征，而不是直接使用的该属性的全部特征。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attr, label, v):\n",
    "        self.attr = attr # 非叶节点的特征名称；叶节点为pi\n",
    "        self.label = label # 叶节点的分类结果；非叶节点为pi\n",
    "        self.attr_v = v  # 从父节点到当前节点的分支特征值\n",
    "        self.children = []  # 当前节点的子节点列表\n",
    "\n",
    "\n",
    "def is_same_on_attr(X, attrs):\n",
    "    '''\n",
    "    数据集X在attrs上的取值是否相同\n",
    "    '''\n",
    "    X_a = X[:, attrs] # 从X中取出剩余特征的部分\n",
    "    target = X_a[0]\n",
    "    for r in range(X_a.shape[0]):\n",
    "        row = X_a[r]\n",
    "        # 第一个样本和任意一个样本中，任意一个特征不同就返回false\n",
    "        if (row != target).any():\n",
    "            return False\n",
    "    # 至此，全部比较完毕，发现所有样本特征都相同，返回true\n",
    "    return True\n",
    "\n",
    "def dicision_tree_init(X, Y, attrs, root, purity_cal):\n",
    "    \"\"\"\n",
    "    递归构建决策树的初始化函数。\n",
    "    - X、Y 训练集\n",
    "    - attrs: 剩余特征列表（索引）\n",
    "    - root: 当前所在的节点\n",
    "    - purity_cal: 计算信息增益\n",
    "    \"\"\"\n",
    "\n",
    "    # 递归基 1: 如果当前数据集 Y 中所有样本的标签相同，则直接设为叶节点\n",
    "    if len(set(Y)) == 1:\n",
    "        root.attr = np.pi\n",
    "        root.label = Y[0]\n",
    "        return None\n",
    "\n",
    "    # 递归基 2: 如果特征用尽，或者数据集在剩余特征上取值相同\n",
    "    if len(attrs) == 0 or is_same_on_attr(X, attrs):\n",
    "        root.attr = np.pi\n",
    "        # Y中出现最多的标签值为叶节点标签\n",
    "        root.label = np.argmax(np.bincount(Y))\n",
    "        return None\n",
    "\n",
    "    # 计算每个特征的划分收益（如信息增益）\n",
    "    purity_attrs = []\n",
    "    for i, a in enumerate(attrs):\n",
    "        # 计算当前特征 a 的纯度值\n",
    "        p = purity_cal(X, Y, a)\n",
    "        purity_attrs.append(p)\n",
    "\n",
    "    # 选择纯度最大的特征作为划分特征\n",
    "    chosen_index = purity_attrs.index(max(purity_attrs))  # 最大纯度的特征索引\n",
    "    chosen_attr = attrs[chosen_index]  # 对应的特征编号\n",
    "\n",
    "    # 设置当前节点为非叶节点，设置其特征\n",
    "    root.attr = chosen_attr\n",
    "    root.label = np.pi\n",
    "\n",
    "    # 从剩余特征中移除已选择的特征\n",
    "    del attrs[chosen_index]\n",
    "\n",
    "    # 获取当前划分特征列\n",
    "    x_attr_col = X[:, chosen_attr]\n",
    "\n",
    "    # 对当前特征的每个取值，生成对应的子节点\n",
    "    for x_v in set(x_attr_col):  # 遍历划分特征的所有取值\n",
    "        # 创建新的子节点，初始化为 -1 表示暂未设置具体值\n",
    "        n = Node(-1, -1, x_v)  # attr = -1, label = -1, attr_v = x_v\n",
    "        root.children.append(n)  # 将子节点添加到当前节点的子节点列表\n",
    "        # 不可能Dv empty 要是empty压根不会在set里\n",
    "        # 选出 X[attr] == x_v的行\n",
    "\n",
    "        # 筛选出在该特征取值下的样本及对应标签\n",
    "        index_x_equal_v = np.where(x_attr_col == x_v)  # 筛选条件\n",
    "        X_x_equal_v = X[index_x_equal_v]  # 筛选后的特征子集\n",
    "        Y_x_equal_v = Y[index_x_equal_v]  # 筛选后的标签子集\n",
    "\n",
    "        # 递归调用构建子树\n",
    "        dicision_tree_init(X_x_equal_v, Y_x_equal_v, attrs, n, purity_cal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用测试集进行预测\n",
    "预测过程较为简单，即，模型从根节点开始，根据样本的特征值选择对应的分支，直到到达叶节点。叶节点存储的分类标签即为预测结果。如果样本的特征值无法匹配树中的某个分支，则预测失败。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicision_tree_predict(x, tree_root):\n",
    "    # 到叶节点了，返回标签\n",
    "    if tree_root.label != np.pi:\n",
    "        return tree_root.label\n",
    "\n",
    "    # 一种错误情况\n",
    "    if tree_root.label == np.pi and tree_root.attr == np.pi:\n",
    "        print(\"err!\")\n",
    "        return None\n",
    "\n",
    "    # 选择当前节点的划分特征\n",
    "    chose_attr = tree_root.attr\n",
    "    for child in tree_root.children:\n",
    "        if child.attr_v == x[chose_attr]:\n",
    "            return dicision_tree_predict(x, child)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ans = load_txt(\"Watermelon-train1.csv\")\n",
    "    X_train1 = ans[:, 1: -1]\n",
    "    Y_train1 = ans[:, -1].astype(np.int64)\n",
    "    test_data = load_txt(\"Watermelon-test1.csv\")\n",
    "    X_test1 = test_data[:, 1:-1]\n",
    "    Y_test1 = test_data[:, -1].astype(np.int64)\n",
    "    r1 = Node(-1, -1, -1)\n",
    "    attrs1 = [0, 1, 2, 3]\n",
    "\n",
    "    dicision_tree_init(X_train1, Y_train1, attrs1, r1, gain)\n",
    "\n",
    "    y_predict = []\n",
    "    for i in range(X_test1.shape[0]):\n",
    "        x = X_test1[i]\n",
    "        y_p = dicision_tree_predict(x, r1)\n",
    "        y_predict.append(y_p)\n",
    "\n",
    "    # 计算准确率\n",
    "    correct_predictions = sum(1 for y_true, y_pred in zip(Y_test1, y_predict) if y_true == y_pred)\n",
    "    total_predictions = len(Y_test1)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果：我们通过信息增益这个指标构建经典的ID3决策树，其在测试集上的准确率为0.7，性能尚可，但仍有改进空间，本身来说，我们的训练集也不是很大，并且没有运用减枝，可能导致过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中级要求\n",
    "#### 连续属性的离散化\n",
    "\n",
    "最简单的策略是采用二分法对连续属性进行处理，这正是C4.5决策树算法中采用的机制。方法就是通过找到最佳的一个分割点，将连续属性划分为两个子集。具体的步骤如下：\n",
    "\n",
    "- 排序：首先将所有样本的连续属性按升序排序。\n",
    "- 计算增益比：对于每一个可能的分割点$t$（位于两个不同样本值之间的中间的任意值），计算信息增益$Gain(D, a, t)$，选择其中的最大值最为我们的划分点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "\n",
    "# 连续属性分割点选择\n",
    "def best_split_continuous(X, Y, attr):\n",
    "    \"\"\"\n",
    "    返回：\n",
    "    - best_split: 最佳分割点的值。\n",
    "    - best_gain_ratio: 在该分割点上的最大增益比值。\n",
    "    \"\"\"\n",
    "    # 获取当前属性列的所有值\n",
    "    X_attr_col = X[:, attr]\n",
    "    unique_values = np.sort(np.unique(X_attr_col))\n",
    "    best_gain_ratio = -float('inf')  \n",
    "    best_split = None\n",
    "\n",
    "    # 遍历所有相邻值之间的中间点作为候选分割点\n",
    "    for i in range(len(unique_values) - 1):\n",
    "        # 当前候选分割点为相邻值的中点\n",
    "        split_point = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "\n",
    "        # 根据分割点将数据划分为左右两部分\n",
    "        X_left = X[X_attr_col <= split_point] \n",
    "        Y_left = Y[X_attr_col <= split_point]\n",
    "        X_right = X[X_attr_col > split_point]\n",
    "        Y_right = Y[X_attr_col > split_point]\n",
    "\n",
    "        # 计算每一部分的权重（样本数量占比）\n",
    "        left_ratio = len(X_left) / len(X)\n",
    "        right_ratio = len(X_right) / len(X)\n",
    "\n",
    "        weighted_entropy = left_ratio * ent(Y_left) + right_ratio * ent(Y_right)\n",
    "        split_info = -left_ratio * np.log2(left_ratio) - right_ratio * np.log2(right_ratio)\n",
    "\n",
    "        # 计算增益比（Gain Ratio）\n",
    "        gain_ratio_val = (ent(Y) - weighted_entropy) / (split_info + 1e-6)  # 避免除零\n",
    "\n",
    "        # 更新最佳增益比及分割点\n",
    "        if gain_ratio_val > best_gain_ratio:\n",
    "            best_gain_ratio = gain_ratio_val\n",
    "            best_split = split_point\n",
    "\n",
    "    return best_split, best_gain_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在数据数据阶段，我们需要分辩究竟哪些属性是“离散”的，哪些是“连续”的，因此，相较于初级要求，我们增加了一个`feature_types`变量，用于识别这个属性的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    \"色泽\": [\"青绿\", \"乌黑\", \"浅白\"],\n",
    "    \"根蒂\": [\"蜷缩\", \"稍蜷\", \"硬挺\"],\n",
    "    \"敲声\": [\"浊响\", \"沉闷\", \"清脆\"],\n",
    "    \"纹理\": [\"清晰\", \"稍糊\", \"模糊\"],\n",
    "    \"密度\": []\n",
    "}\n",
    "feature_types = {\n",
    "    0: \"discrete\",\n",
    "    1: \"discrete\",\n",
    "    2: \"discrete\",\n",
    "    3: \"discrete\",\n",
    "    4: \"discrete\",\n",
    "    5: \"continuous\",\n",
    "}\n",
    "\n",
    "lable_list = [\"否\", \"是\"]\n",
    "feature_list = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"密度\"]\n",
    "\n",
    "def load_txt(path):\n",
    "    ans = []\n",
    "    with codecs.open(path, \"r\", \"GBK\") as f:\n",
    "        line = f.readline()\n",
    "        line = f.readline()\n",
    "        while line:\n",
    "            d = line.rstrip(\"\\r\\n\").split(',')\n",
    "            re = []\n",
    "            re.append(int(d[0]))\n",
    "            re.append(feature_dict.get(\"色泽\").index(d[1]) if d[1] in feature_dict.get(\"色泽\") else float(d[1]))\n",
    "            re.append(feature_dict.get(\"根蒂\").index(d[2]) if d[2] in feature_dict.get(\"根蒂\") else float(d[2]))\n",
    "            re.append(feature_dict.get(\"敲声\").index(d[3]) if d[3] in feature_dict.get(\"敲声\") else float(d[3]))\n",
    "            re.append(feature_dict.get(\"纹理\").index(d[4]) if d[4] in feature_dict.get(\"纹理\") else float(d[4]))\n",
    "            re.append(float(d[5]))\n",
    "            re.append(lable_list.index(d[-1]))\n",
    "            ans.append(np.array(re))\n",
    "            line = f.readline()\n",
    "    return np.array(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C4.5 决策树相关理论\n",
    "\n",
    "C4.5 是 ID3 决策树算法的扩展和优化版本。与ID3的最主要区别就是，C4.5 使用“信息增益比”作为属性选择标准，而不是“信息增益”。信息增益有一个问题：它倾向于选择取值较多的特征，为了解决这个问题，C4.5 引入了“信息增益比”：\n",
    "$$\n",
    "\\text{Gain Ratio}(D, A) = \\frac{\\text{Gain}(D, A)}{\\text{SplitInfo}(D, A)}\n",
    "$$\n",
    "其中：\n",
    "$$\n",
    "\\text{SplitInfo}(D, A) = - \\sum_{v \\in \\text{Values}(A)} \\frac{|D_v|}{|D|} \\cdot \\log_2 \\frac{|D_v|}{|D|}\n",
    "$$\n",
    "信息增益比通过惩罚划分的复杂性，优先选择能够平衡信息增益和分裂数的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算信息熵\n",
    "def ent(Y):\n",
    "    unique, counts = np.unique(Y, return_counts=True)\n",
    "    prob = counts / len(Y)\n",
    "    return -np.sum(prob * np.log2(prob))\n",
    "\n",
    "# 计算信息增益\n",
    "def gain_ratio(X, Y, attr):\n",
    "    X_attr_col = X[:, attr]\n",
    "    unique_values = np.unique(X_attr_col)\n",
    "    entropy_d = ent(Y)\n",
    "    weighted_entropy = 0\n",
    "    split_info = 0\n",
    "    for value in unique_values:\n",
    "        index = np.where(X_attr_col == value)\n",
    "        Y_sub = Y[index]\n",
    "        weighted_entropy += len(Y_sub) / len(Y) * ent(Y_sub)\n",
    "        split_info -= len(Y_sub) / len(Y) * np.log2(len(Y_sub) / len(Y))\n",
    "    \n",
    "    # Gain Ratio = (信息增益) / (特征划分的熵)\n",
    "    gain = entropy_d - weighted_entropy\n",
    "    return gain / (split_info + 1e-6)  # 防止除以0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相较于ID3决策树，决策树的构建算法差别不大，唯二区别在于：需要处理连续值（仅分为左右子树）；最佳特征选择的标准不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attr, label, v):\n",
    "        self.attr = attr\n",
    "        self.label = label\n",
    "        self.attr_v = v\n",
    "        self.children = []\n",
    "def dicision_tree_init(X, Y, attrs, root, purity_cal):\n",
    "    # 递归基 1: 如果当前数据集的所有样本的标签相同，直接设置为叶节点\n",
    "    if len(set(Y)) == 1:\n",
    "        root.attr = np.pi \n",
    "        root.label = Y[0] \n",
    "        return\n",
    "    # 递归基 2: 如果没有剩余的可用特征，设置叶节点标签为样本中出现最多的类别\n",
    "    if len(attrs) == 0:\n",
    "        root.attr = np.pi\n",
    "        root.label = np.argmax(np.bincount(Y))\n",
    "        return\n",
    "\n",
    "    # 计算每个特征的纯度（如信息增益或增益率）\n",
    "    purity_attrs = []  # 存储每个特征的纯度\n",
    "    for i, a in enumerate(attrs):\n",
    "        if feature_types[a] == \"continuous\":\n",
    "            best_split, gain_ratio_val = best_split_continuous(X, Y, a)\n",
    "            purity_attrs.append(gain_ratio_val)\n",
    "        else:\n",
    "            purity_attrs.append(purity_cal(X, Y, a))\n",
    "\n",
    "    # 选择纯度最高的特征作为当前节点的划分标准\n",
    "    chosen_index = purity_attrs.index(max(purity_attrs))\n",
    "    chosen_attr = attrs[chosen_index]\n",
    "\n",
    "    # 设置当前节点的划分特征\n",
    "    root.attr = chosen_attr\n",
    "    root.label = np.pi\n",
    "    del attrs[chosen_index] \n",
    "\n",
    "    # 如果是连续值特征\n",
    "    if feature_types[chosen_attr] == \"continuous\":\n",
    "        # 获取最佳分割点\n",
    "        best_split, _ = best_split_continuous(X, Y, chosen_attr)\n",
    "        root.attr_v = best_split  # 当前节点记录分割点\n",
    "\n",
    "        # 将数据集划分为两个子集（<= 分割点 和 > 分割点）\n",
    "        left_index = np.where(X[:, chosen_attr] <= best_split)\n",
    "        right_index = np.where(X[:, chosen_attr] > best_split)\n",
    "        X_left = X[left_index]\n",
    "        Y_left = Y[left_index]\n",
    "        X_right = X[right_index]\n",
    "        Y_right = Y[right_index]\n",
    "\n",
    "        # 创建左右子节点\n",
    "        left_node = Node(-1, -1, \"left\")  \n",
    "        right_node = Node(-1, -1, \"right\")\n",
    "        root.children.append(left_node) \n",
    "        root.children.append(right_node)\n",
    "\n",
    "        # 递归构建左右子树\n",
    "        dicision_tree_init(X_left, Y_left, attrs, left_node, purity_cal)\n",
    "        dicision_tree_init(X_right, Y_right, attrs, right_node, purity_cal)\n",
    "    else:\n",
    "        # 如果是离散值特征\n",
    "        x_attr_col = X[:, chosen_attr]  # 当前特征的所有取值\n",
    "        for x_v in set(x_attr_col):  # 遍历特征的所有可能取值\n",
    "            n = Node(-1, -1, x_v)  # 创建子节点，标记分支特征值为 x_v\n",
    "            root.children.append(n)  # 添加到当前节点的子节点列表中\n",
    "\n",
    "            # 筛选出当前特征值等于 x_v 的子集\n",
    "            index_x_equal_v = np.where(x_attr_col == x_v)\n",
    "            X_x_equal_v = X[index_x_equal_v]\n",
    "            Y_x_equal_v = Y[index_x_equal_v]\n",
    "\n",
    "            # 递归构建子树\n",
    "            dicision_tree_init(X_x_equal_v, Y_x_equal_v, attrs, n, purity_cal)\n",
    "\n",
    "def dicision_tree_predict1(x, tree_root):\n",
    "    if tree_root.label != np.pi:\n",
    "        return tree_root.label\n",
    "    if tree_root.label == np.pi and tree_root.attr == np.pi:\n",
    "        print(\"err!\")\n",
    "        return None\n",
    "    chose_attr = tree_root.attr\n",
    "    if feature_types[chose_attr] == \"continuous\":\n",
    "        if x[chose_attr] <= tree_root.attr_v:\n",
    "            return dicision_tree_predict1(x, tree_root.children[0])\n",
    "        else:\n",
    "            return dicision_tree_predict1(x, tree_root.children[1])\n",
    "    else:\n",
    "        for child in tree_root.children:\n",
    "            if child.attr_v == x[chose_attr]:\n",
    "                return dicision_tree_predict1(x, child)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ans = load_txt(\"Watermelon-train2.csv\")\n",
    "    X_train2 = ans[:, 1: -1]\n",
    "    Y_train2 = ans[:, -1].astype(np.int64)\n",
    "    test_data = load_txt(\"Watermelon-test2.csv\")\n",
    "    X_test2 = test_data[:, 1:-1]\n",
    "    Y_test2 = test_data[:, -1].astype(np.int64)\n",
    "    \n",
    "    r2 = Node(-1, -1, -1)\n",
    "    attrs2 = [0, 1, 2, 3, 4]\n",
    "\n",
    "    dicision_tree_init(X_train2, Y_train2, attrs2, r2, gain_ratio)\n",
    "\n",
    "    y_predict2 = []\n",
    "    for i in range(X_test2.shape[0]):\n",
    "        x = X_test2[i]\n",
    "        y_p = dicision_tree_predict1(x, r2)\n",
    "        y_predict2.append(y_p)\n",
    "\n",
    "    correct_predictions = sum(1 for y_true, y_pred in zip(Y_test2, y_predict2) if y_true == y_pred)\n",
    "    total_predictions = len(Y_test2)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果：我们通过信息增益比这个指标构建C4.5决策树，其在测试集上的准确率为0.6，表明模型的性能还有一定提升空间，较于仅处理离散属性的ID3决策树，连续变量的引入本身为模型处理带来了更高的难度，准确率下降应该是情理之中的，同时，训练集和测试集样本量确实较少。这个结果向我们警示了运用减枝的必要性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高级要求"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剪枝通常分为预剪枝和后剪枝。其中，预剪枝是在构建决策树的过程中，通过设定某些条件（例如树的深度、节点样本数或信息增益等）提前停止分裂，避免树过度生长，减少过拟合的风险。而后剪枝则是在决策树完全构建后，基于验证集的性能对已生成的树进行修剪，去除那些对模型泛化能力没有贡献或会导致过拟合的分支。后剪枝通过评估每个子树的贡献，对其进行剪枝，简化树结构并提高其对新数据的泛化能力。\n",
    "\n",
    "按照题目要求，我们要对构建好的决策树进行剪枝，因此属于后剪枝。我的方法是采用后剪枝算法，首先基于我们构建好的决策树，然后通过计算每个子树在验证集上的准确率，对那些剪枝后准确率不降低的子树进行剪枝。具体地，我通过计算每个节点子树的预测准确率，并与剪枝后的叶节点的准确率进行比较，如果剪枝后的准确率等于或高于子树的准确率，则进行剪枝，最后生成剪枝后的决策树。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_pruning(tree_root, X_val, Y_val):\n",
    "    if tree_root.label != np.pi:\n",
    "        return tree_root  # 当前节点是叶节点，无需剪枝\n",
    "    # 递归对子节点进行剪枝\n",
    "    for child in tree_root.children:\n",
    "        post_pruning(child, X_val, Y_val)\n",
    "    # 计算剪枝前的误差\n",
    "    original_error = calculate_error(tree_root, X_val, Y_val)\n",
    "    # 保存当前节点信息并尝试剪枝\n",
    "    original_label = tree_root.label\n",
    "    tree_root.label = np.argmax(np.bincount(Y_val))  # 将当前节点转换为叶节点\n",
    "    pruned_error = calculate_error(tree_root, X_val, Y_val)\n",
    "    print(f\"尝试剪枝节点: 属性={tree_root.attr}, 原误差={original_error:.4f}, 剪枝后误差={pruned_error:.4f}\")\n",
    "    # 如果剪枝后误差降低或不变，确认剪枝\n",
    "    if pruned_error <= original_error:\n",
    "        print(f\"剪枝成功: 将属性为 {tree_root.attr} 的节点剪枝为叶节点，标签为 {tree_root.label}\")\n",
    "        tree_root.children = []  # 移除子树\n",
    "    else:\n",
    "        print(f\"剪枝失败: 恢复属性为 {tree_root.attr} 的节点为非叶节点\")\n",
    "        tree_root.label = original_label  # 恢复为非叶节点\n",
    "    return tree_root\n",
    "\n",
    "# 计算误差\n",
    "def calculate_error(tree_root, X_val, Y_val):\n",
    "    predictions = []\n",
    "    for i in range(X_val.shape[0]):\n",
    "        x = X_val[i]\n",
    "        y_pred = dicision_tree_predict1(x, tree_root)\n",
    "        predictions.append(y_pred)\n",
    "    predictions = np.array(predictions)\n",
    "    error = np.sum(predictions != Y_val) / len(Y_val)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3决策树剪枝过程：\n",
      "尝试剪枝节点: 属性=1, 原误差=0.4000, 剪枝后误差=0.5000\n",
      "剪枝失败: 恢复属性为 1 的节点为非叶节点\n",
      "尝试剪枝节点: 属性=2, 原误差=0.7000, 剪枝后误差=0.5000\n",
      "剪枝成功: 将属性为 2 的节点剪枝为叶节点，标签为 0\n",
      "尝试剪枝节点: 属性=0, 原误差=0.5000, 剪枝后误差=0.5000\n",
      "剪枝成功: 将属性为 0 的节点剪枝为叶节点，标签为 0\n",
      "尝试剪枝节点: 属性=3, 原误差=0.2000, 剪枝后误差=0.5000\n",
      "剪枝失败: 恢复属性为 3 的节点为非叶节点\n",
      "\n",
      "C4.5决策树剪枝过程：\n",
      "尝试剪枝节点: 属性=4, 原误差=0.8000, 剪枝后误差=0.4000\n",
      "剪枝成功: 将属性为 4 的节点剪枝为叶节点，标签为 1\n",
      "尝试剪枝节点: 属性=1, 原误差=0.2000, 剪枝后误差=0.4000\n",
      "剪枝失败: 恢复属性为 1 的节点为非叶节点\n",
      "尝试剪枝节点: 属性=0, 原误差=0.4000, 剪枝后误差=0.4000\n",
      "剪枝成功: 将属性为 0 的节点剪枝为叶节点，标签为 1\n",
      "尝试剪枝节点: 属性=2, 原误差=0.2000, 剪枝后误差=0.4000\n",
      "剪枝失败: 恢复属性为 2 的节点为非叶节点\n",
      "尝试剪枝节点: 属性=3, 原误差=0.0000, 剪枝后误差=0.4000\n",
      "剪枝失败: 恢复属性为 3 的节点为非叶节点\n",
      "\n",
      "决策树1的准确率（剪枝后）: 0.8\n",
      "决策树2的准确率（剪枝后）: 1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #后剪枝处理\n",
    "    # post_pruning(r1, X_train1, Y_train1)\n",
    "    # print()\n",
    "    # post_pruning(r2, X_train2, Y_train2)\n",
    "    # print()\n",
    "\n",
    "    print(\"ID3决策树剪枝过程：\")\n",
    "    post_pruning(r1, X_test1, Y_test1)\n",
    "    print()\n",
    "    print(\"C4.5决策树剪枝过程：\")\n",
    "    post_pruning(r2, X_test2, Y_test2)\n",
    "    print()\n",
    "\n",
    "\n",
    "    # 对测试集进行预测并计算准确率\n",
    "    def calculate_accuracy(X_test, Y_test, tree):\n",
    "        y_predict = []\n",
    "        for i in range(X_test.shape[0]):\n",
    "            x = X_test[i]\n",
    "            y_p = dicision_tree_predict1(x, tree)\n",
    "            y_predict.append(y_p)\n",
    "        correct_predictions = sum(1 for y_true, y_pred in zip(Y_test, y_predict) if y_true == y_pred)\n",
    "        total_predictions = len(Y_test)\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        return accuracy\n",
    "\n",
    "    # 计算并输出准确率\n",
    "    accuracy1 = calculate_accuracy(X_test1, Y_test1, r1)\n",
    "    accuracy2 = calculate_accuracy(X_test2, Y_test2, r2)\n",
    "\n",
    "    print(\"决策树1的准确率（剪枝后）:\", accuracy1)\n",
    "    print(\"决策树2的准确率（剪枝后）:\", accuracy2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过输出，我们可以详细了解整个剪枝过程，可以看出，ID3和C4.5决策树都经历了多个节点的尝试剪枝。\n",
    "\n",
    "对于**ID3决策树**，成功剪枝了属性为2和0的节点。这两个节点在剪枝后误差降低或没有显著增加，因此被剪枝为叶节点，标签分别为0。其余节点，如属性为1和3的节点，由于剪枝后误差增加，未能成功剪枝，这些节点保持为非叶节点。\n",
    "\n",
    "对于**C4.5决策树**，成功剪枝了属性为4和0的节点，剪枝后误差有所减少或没有增加，因此将它们转换为叶节点，标签分别为1。其余的节点，如属性为1、2和3的节点，虽然在剪枝时误差有所变化，但并未带来有效的误差减少，因此保持为非叶节点。\n",
    "\n",
    "两棵决策树的剪枝过程成功地简化了部分节点，提升了模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
